{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import random\n",
    "import tabulate\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "\n",
    "import data\n",
    "import models\n",
    "import utils\n",
    "import regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalArguments():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model       = 'ConvFCSimple'\n",
    "        self.dataset     = 'CIFAR100'\n",
    "        self.data_path   = '../Data/'\n",
    "        self.batch_size  = 128\n",
    "        self.num_workers = 4\n",
    "        self.transform   = 'VGG'\n",
    "        self.use_test    = True\n",
    "        self.ckpt        = '../Checkpoints/ConvFCSimple/CIFAR100_STEP200/0/checkpoint-200.pt'\n",
    "        self.device      = 0\n",
    "        self.seed        = 0\n",
    "        self.dir         = '../Checkpoints/test'\n",
    "        self.regularizer = None\n",
    "        \n",
    "        self.momentum    = 0.9\n",
    "        self.wd          = 1e-4\n",
    "        self.cycle       = 200\n",
    "        self.epochs      = 1600\n",
    "        self.lr_1        = 0.005\n",
    "        self.lr_2        = 0.0001\n",
    "        self.version     = 'classic'\n",
    "        self.boost_lr    = 'auto'\n",
    "        self.scheduler   = 'slide'\n",
    "        self.independent = True\n",
    "        \n",
    "args = GlobalArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert args.cycle % 2 == 0, 'Cycle length should be even'\n",
    "\n",
    "os.makedirs(args.dir, exist_ok=True)\n",
    "with open(os.path.join(args.dir, 'fge.sh'), 'w') as f:\n",
    "    f.write(' '.join(sys.argv))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "if args.seed == 0:\n",
    "    args.seed = random.randint(0, 1000000)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "device = 'cuda:' + str(args.device) if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(device)\n",
    "print ('Device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (args.boost_lr == 'auto'):\n",
    "    boost_lr = 1.0\n",
    "else:\n",
    "    boost_lr = float(args.boost_lr)\n",
    "\n",
    "if   args.dataset == \"CIFAR10\":\n",
    "    num_classes = 10\n",
    "elif args.dataset == \"CIFAR100\":\n",
    "    num_classes = 100\n",
    "\n",
    "if   args.version == 'classic':\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "elif args.version == 'simple':\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "else:\n",
    "    raise AssertionError('I don`t know this implementation of gradient boosting')\n",
    "\n",
    "if   args.scheduler == 'cyclic':\n",
    "    scheduler = utils.cyclic_learning_rate\n",
    "elif args.scheduler == 'linear':\n",
    "    scheduler = utils.linear_learning_rate\n",
    "elif args.scheduler == 'slide':\n",
    "    scheduler = utils.slide_learning_rate\n",
    "else:\n",
    "    raise AssertionError('I don`t know such scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 34, 34]             224\n",
      "              ReLU-2            [-1, 8, 34, 34]               0\n",
      "         MaxPool2d-3            [-1, 8, 32, 32]               0\n",
      "            Conv2d-4            [-1, 8, 32, 32]             584\n",
      "              ReLU-5            [-1, 8, 32, 32]               0\n",
      "         MaxPool2d-6            [-1, 8, 15, 15]               0\n",
      "            Conv2d-7            [-1, 8, 15, 15]             584\n",
      "              ReLU-8            [-1, 8, 15, 15]               0\n",
      "         MaxPool2d-9              [-1, 8, 7, 7]               0\n",
      "           Linear-10                  [-1, 100]          39,300\n",
      "             ReLU-11                  [-1, 100]               0\n",
      "           Linear-12                  [-1, 100]          10,100\n",
      "================================================================\n",
      "Total params: 50,792\n",
      "Trainable params: 50,792\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.38\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "architecture = getattr(models, args.model)\n",
    "model = architecture.base(num_classes=num_classes, **architecture.kwargs)\n",
    "\n",
    "checkpoint = torch.load(args.ckpt)\n",
    "# start_epoch = checkpoint['epoch'] + 1\n",
    "start_epoch = checkpoint['epoch']\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.cuda()\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Initial logits :\n",
      "Shape : torch.Size([50000, 100]) Logits_mean : 0.03137224540114403\n",
      "Max : 27.365541458129883 Min : -31.16400718688965\n",
      "You are going to run models on the test set. Are you sure?\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "loaders, num_classes = data.loaders_gb(\n",
    "    args.dataset,\n",
    "    args.data_path,\n",
    "    args.batch_size,\n",
    "    args.num_workers,\n",
    "    args.transform,\n",
    "    args.use_test,\n",
    "    shuffle_train=True,\n",
    "    logits_generator=regularization.dataset_logits_generator(\n",
    "        model,\n",
    "        transform=getattr(getattr(data.Transforms, args.dataset), args.transform).train,\n",
    "        batch_size=args.batch_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
      "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "             ReLU-10          [-1, 128, 16, 16]               0\n",
      "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
      "             ReLU-13          [-1, 128, 16, 16]               0\n",
      "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
      "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
      "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
      "             ReLU-17            [-1, 256, 8, 8]               0\n",
      "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
      "             ReLU-20            [-1, 256, 8, 8]               0\n",
      "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
      "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
      "             ReLU-23            [-1, 256, 8, 8]               0\n",
      "        MaxPool2d-24            [-1, 256, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 4]       1,180,160\n",
      "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-27            [-1, 512, 4, 4]               0\n",
      "           Conv2d-28            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-30            [-1, 512, 4, 4]               0\n",
      "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-33            [-1, 512, 4, 4]               0\n",
      "        MaxPool2d-34            [-1, 512, 2, 2]               0\n",
      "           Conv2d-35            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-37            [-1, 512, 2, 2]               0\n",
      "           Conv2d-38            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-40            [-1, 512, 2, 2]               0\n",
      "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
      "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-43            [-1, 512, 2, 2]               0\n",
      "        MaxPool2d-44            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-45            [-1, 512, 7, 7]               0\n",
      "           Linear-46                 [-1, 4096]     102,764,544\n",
      "             ReLU-47                 [-1, 4096]               0\n",
      "          Dropout-48                 [-1, 4096]               0\n",
      "           Linear-49                 [-1, 4096]      16,781,312\n",
      "             ReLU-50                 [-1, 4096]               0\n",
      "          Dropout-51                 [-1, 4096]               0\n",
      "           Linear-52                  [-1, 100]         409,700\n",
      "================================================================\n",
      "Total params: 134,678,692\n",
      "Trainable params: 134,678,692\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.95\n",
      "Params size (MB): 513.76\n",
      "Estimated Total Size (MB): 520.72\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "architecture = getattr(models, 'vgg16_bn')\n",
    "model = architecture.base(num_classes=num_classes, **architecture.kwargs)\n",
    "model.cuda()\n",
    "summary(model, (3, 32, 32))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=args.lr_1,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.wd\n",
    ")\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inintial accuracy : tensor(0.0089)\n"
     ]
    }
   ],
   "source": [
    "# test_res = utils.test(loaders['test'], model, criterion)\n",
    "# print ('Initial quality: ', test_res['accuracy'])\n",
    "\n",
    "ensemble_size = 0\n",
    "predictions_sum = np.zeros((len(loaders['test'].dataset), num_classes))\n",
    "\n",
    "columns = ['ep', 'lr', 'tr_loss', 'tr_acc', 'te_nll', 'te_loss', 'te_acc', 'ens_acc', 'time']\n",
    "\n",
    "if args.regularizer is None:\n",
    "    regularizer = None\n",
    "elif args.regularizer == 'MSE2':\n",
    "    regularizer = regularization.TwoModelsMSE(model, args.reg_wd).reg\n",
    "\n",
    "utils.save_checkpoint(\n",
    "    args.dir,\n",
    "    start_epoch,\n",
    "    name='fge',\n",
    "    model_state=model.state_dict(),\n",
    "    optimizer_state=optimizer.state_dict(),\n",
    "    boost_weight=1.)\n",
    "\n",
    "logits_sum, targets = utils.logits(loaders['test'], model)\n",
    "print ('Inintial accuracy :', torch.eq(logits_sum.argmax(dim=1), targets).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  ep         lr    tr_loss     tr_acc     te_nll    te_loss     te_acc  ens_acc         time\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "   0   0.005000   0.019008  42.860000   0.007442   2.325694  39.590000             27.130669\n",
      "   1   0.005000   0.007223  42.908000   0.007413   2.325687  39.600000             26.195138\n",
      "   2   0.005000   0.007140  42.966000   0.007405   2.325686  39.610000             26.354546\n",
      "   3   0.005000   0.007113  42.948000   0.007402   2.325685  39.590000             26.208734\n",
      "   4   0.005000   0.007099  42.948000   0.007400   2.325689  39.580000             26.174862\n",
      "   5   0.005000   0.007091  42.938000   0.007399   2.325677  39.590000             26.249727\n",
      "   6   0.005000   0.007086  42.974000   0.007398   2.325670  39.570000             26.224368\n",
      "   7   0.005000   0.007083  42.956000   0.007398   2.325679  39.580000             26.135436\n",
      "   8   0.005000   0.007081  42.968000   0.007397   2.325674  39.570000             26.195068\n",
      "   9   0.005000   0.007079  42.950000   0.007397   2.325671  39.580000             26.282746\n",
      "  10   0.005000   0.007077  42.960000   0.007397   2.325680  39.590000             26.262096\n",
      "  11   0.005000   0.007076  42.960000   0.007397   2.325678  39.580000             26.358784\n",
      "  12   0.005000   0.007076  42.948000   0.007396   2.325681  39.580000             26.211415\n",
      "  13   0.005000   0.007074  42.972000   0.007396   2.325678  39.580000             26.371309\n",
      "  14   0.005000   0.007074  42.956000   0.007396   2.325678  39.580000             26.241370\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    time_ep = time.time()\n",
    "    lr_schedule = scheduler(epoch, args.cycle, args.lr_1, args.lr_2)\n",
    "    \n",
    "    train_res = utils.train_gb(\n",
    "        loaders['train'],\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        lr_schedule=lr_schedule,\n",
    "        regularizer=regularizer,\n",
    "        gb_version=args.version,\n",
    "        boost_lr=boost_lr)\n",
    "    test_res = utils.test_gb(\n",
    "        loaders['test'],\n",
    "        model,\n",
    "        criterion,\n",
    "        boost_lr=boost_lr)\n",
    "    time_ep = time.time() - time_ep\n",
    "    ens_acc = None\n",
    "\n",
    "    if (epoch + 1) % args.cycle == 0:\n",
    "        if args.boost_lr == 'auto':\n",
    "            os.makedirs(args.dir + '/boost_lr', exist_ok=True)\n",
    "            boost_lr = regularization.adjust_boost_lr(\n",
    "                loaders['train'],\n",
    "                model,\n",
    "                save_info=args.dir + '/boost_lr/' + str(epoch) + '.pt')\n",
    "        print ('Boost_lr : ', boost_lr)\n",
    "        ensemble_size += 1\n",
    "        logits, targets = utils.logits(loaders['test'], model)\n",
    "        logits_sum += boost_lr * logits\n",
    "        ens_acc = 100.0 * torch.eq(logits_sum.argmax(dim=1), targets).float().mean().item()\n",
    "        \n",
    "        regularization.logits_info(logits, logits_sum=logits_sum)\n",
    "        \n",
    "        utils.save_checkpoint(\n",
    "            args.dir,\n",
    "            start_epoch + epoch,\n",
    "            name='fge',\n",
    "            model_state=model.state_dict(),\n",
    "            optimizer_state=optimizer.state_dict(),\n",
    "            boost_weight=boost_lr\n",
    "        )\n",
    "\n",
    "#     if args.regularizer is not None and (epoch + 1) % (args.cycle) == 0:\n",
    "#         regularizer = regularization.TwoModelsMSE(model, args.reg_wd).reg\n",
    "#     if args.regularizer is not None and (epoch + 1) % (args.cycle // 2) == args.cycle // 2:\n",
    "#         regularizer = None\n",
    "\n",
    "#     if args.weighted_samples is not None and (epoch + 1) % args.cycle == 0:\n",
    "#     if (epoch + 1) % args.cycle == 0:\n",
    "        loaders['train'].dataset.update_logits(\n",
    "            boost_lr,\n",
    "            logits_generator=regularization.dataset_logits_generator(\n",
    "                model,\n",
    "                transform=getattr(getattr(\n",
    "                        data.Transforms,\n",
    "                        args.dataset),\n",
    "                    args.transform).train,\n",
    "                batch_size = args.batch_size))\n",
    "        loaders['test'].dataset.update_logits(\n",
    "            boost_lr,\n",
    "            logits_generator=regularization.dataset_logits_generator(\n",
    "                model,\n",
    "                transform=getattr(getattr(\n",
    "                        data.Transforms,\n",
    "                        args.dataset),\n",
    "                    args.transform).test,\n",
    "                batch_size = args.batch_size))\n",
    "        \n",
    "        if args.independent:\n",
    "            print (\"I am making a new model\")\n",
    "            model = architecture.base(num_classes=num_classes, **architecture.kwargs)\n",
    "            model.cuda()\n",
    "            optimizer = torch.optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=args.lr_1,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd\n",
    "            )\n",
    "        \n",
    "    values = [epoch, lr_schedule(1.0), train_res['loss'], train_res['accuracy'], test_res['nll'], test_res['loss'], test_res['accuracy'], ens_acc, time_ep]\n",
    "    table = tabulate.tabulate([values], columns, tablefmt='simple', floatfmt='9.6f')\n",
    "    if epoch % 40 == 0:\n",
    "        table = table.split('\\n')\n",
    "        table = '\\n'.join([table[1]] + table)\n",
    "    else:\n",
    "        table = table.split('\\n')[2]\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

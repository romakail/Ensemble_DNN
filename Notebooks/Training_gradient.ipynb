{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import random\n",
    "import tabulate\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "\n",
    "import data\n",
    "import models\n",
    "import utils\n",
    "import regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalArguments():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model       = 'ConvFCSimple'\n",
    "        self.dataset     = 'CIFAR100'\n",
    "        self.data_path   = '../Data/'\n",
    "        self.batch_size  = 128\n",
    "        self.num_workers = 4\n",
    "        self.transform   = 'VGG'\n",
    "        self.use_test    = True\n",
    "        self.ckpt        = '../Checkpoints/ConvFCSimple/CIFAR100_STEP200/0/checkpoint-200.pt'\n",
    "        self.device      = 0\n",
    "        self.seed        = 0\n",
    "        self.dir         = '../Checkpoints/FGE/CIFAR100/ConvFCSimpleTanh/ind_gb/classic/0'\n",
    "        self.regularizer = None\n",
    "        \n",
    "        self.momentum    = 0.9\n",
    "        self.wd          = 1e-4\n",
    "        self.cycle       = 200\n",
    "        self.epochs      = 1600\n",
    "        self.lr_1        = 0.005\n",
    "        self.lr_2        = 0.0001\n",
    "        self.version     = 'classic'\n",
    "        self.boost_lr    = 'auto'\n",
    "        self.scheduler   = 'slide'\n",
    "        self.independent = True\n",
    "        \n",
    "args = GlobalArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r ../Checkpoints/FGE/CIFAR100/ConvFCSimpleTanh/ind_gb/classic/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert args.cycle % 2 == 0, 'Cycle length should be even'\n",
    "\n",
    "os.makedirs(args.dir, exist_ok=False)\n",
    "with open(os.path.join(args.dir, 'fge.sh'), 'w') as f:\n",
    "    f.write(' '.join(sys.argv))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "if args.seed == 0:\n",
    "    args.seed = random.randint(0, 1000000)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "device = 'cuda:' + str(args.device) if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.set_device(device)\n",
    "print ('Device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (args.boost_lr == 'auto'):\n",
    "    boost_lr = 1.0\n",
    "else:\n",
    "    boost_lr = float(args.boost_lr)\n",
    "\n",
    "if   args.dataset == \"CIFAR10\":\n",
    "    num_classes = 10\n",
    "elif args.dataset == \"CIFAR100\":\n",
    "    num_classes = 100\n",
    "\n",
    "if   args.version == 'classic':\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "elif args.version == 'simple':\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "else:\n",
    "    raise AssertionError('I don`t know this implementation of gradient boosting')\n",
    "\n",
    "if   args.scheduler == 'cyclic':\n",
    "    scheduler = utils.cyclic_learning_rate\n",
    "elif args.scheduler == 'linear':\n",
    "    scheduler = utils.linear_learning_rate\n",
    "elif args.scheduler == 'slide':\n",
    "    scheduler = utils.slide_learning_rate\n",
    "else:\n",
    "    raise AssertionError('I don`t know such scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 34, 34]             224\n",
      "              ReLU-2            [-1, 8, 34, 34]               0\n",
      "         MaxPool2d-3            [-1, 8, 32, 32]               0\n",
      "            Conv2d-4            [-1, 8, 32, 32]             584\n",
      "              ReLU-5            [-1, 8, 32, 32]               0\n",
      "         MaxPool2d-6            [-1, 8, 15, 15]               0\n",
      "            Conv2d-7            [-1, 8, 15, 15]             584\n",
      "              ReLU-8            [-1, 8, 15, 15]               0\n",
      "         MaxPool2d-9              [-1, 8, 7, 7]               0\n",
      "           Linear-10                  [-1, 100]          39,300\n",
      "             ReLU-11                  [-1, 100]               0\n",
      "           Linear-12                  [-1, 100]          10,100\n",
      "================================================================\n",
      "Total params: 50,792\n",
      "Trainable params: 50,792\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.38\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "architecture = getattr(models, args.model)\n",
    "model = architecture.base(num_classes=num_classes, **architecture.kwargs)\n",
    "\n",
    "checkpoint = torch.load(args.ckpt)\n",
    "# start_epoch = checkpoint['epoch'] + 1\n",
    "start_epoch = checkpoint['epoch']\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.cuda()\n",
    "\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Initial logits :\n",
      "Shape : torch.Size([50000, 100]) Logits_mean : 0.03155555948615074\n",
      "Max : 26.66777992248535 Min : -30.140625\n",
      "You are going to run models on the test set. Are you sure?\n",
      "Files already downloaded and verified\n",
      "Inintial accuracy : tensor(0.4237)\n"
     ]
    }
   ],
   "source": [
    "loaders, num_classes = data.loaders_gb(\n",
    "    args.dataset,\n",
    "    args.data_path,\n",
    "    args.batch_size,\n",
    "    args.num_workers,\n",
    "    args.transform,\n",
    "    args.use_test,\n",
    "    shuffle_train=True,\n",
    "    logits_generator=regularization.dataset_logits_generator(\n",
    "        model,\n",
    "        transform=getattr(getattr(data.Transforms, args.dataset), args.transform).train,\n",
    "        batch_size=args.batch_size),\n",
    ")\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=args.lr_1,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.wd\n",
    ")\n",
    "utils.save_checkpoint(\n",
    "    args.dir,\n",
    "    start_epoch,\n",
    "    name='fge',\n",
    "    model_state=model.state_dict(),\n",
    "    optimizer_state=optimizer.state_dict(),\n",
    "    boost_weight=1.)\n",
    "\n",
    "logits_sum, targets = utils.logits(loaders['test'], model)\n",
    "print ('Inintial accuracy :', torch.eq(logits_sum.argmax(dim=1), targets).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 34, 34]             224\n",
      "              ReLU-2            [-1, 8, 34, 34]               0\n",
      "         MaxPool2d-3            [-1, 8, 32, 32]               0\n",
      "            Conv2d-4            [-1, 8, 32, 32]             584\n",
      "              ReLU-5            [-1, 8, 32, 32]               0\n",
      "         MaxPool2d-6            [-1, 8, 15, 15]               0\n",
      "            Conv2d-7            [-1, 8, 15, 15]             584\n",
      "              ReLU-8            [-1, 8, 15, 15]               0\n",
      "         MaxPool2d-9              [-1, 8, 7, 7]               0\n",
      "           Linear-10                  [-1, 100]          39,300\n",
      "             ReLU-11                  [-1, 100]               0\n",
      "           Linear-12                  [-1, 100]          10,100\n",
      "             Tanh-13                  [-1, 100]               0\n",
      "================================================================\n",
      "Total params: 50,792\n",
      "Trainable params: 50,792\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.38\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "architecture = getattr(models, 'ConvFCSimpleTanh')\n",
    "model = architecture.base(num_classes=num_classes, **architecture.kwargs)\n",
    "model.cuda()\n",
    "summary(model, (3, 32, 32))\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=args.lr_1,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.wd\n",
    ")\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_res = utils.test(loaders['test'], model, criterion)\n",
    "# print ('Initial quality: ', test_res['accuracy'])\n",
    "\n",
    "ensemble_size = 0\n",
    "predictions_sum = np.zeros((len(loaders['test'].dataset), num_classes))\n",
    "\n",
    "columns = ['ep', 'lr', 'tr_loss', 'tr_acc', 'te_nll', 'te_loss', 'te_acc', 'ens_acc', 'time']\n",
    "\n",
    "if args.regularizer is None:\n",
    "    regularizer = None\n",
    "elif args.regularizer == 'MSE2':\n",
    "    regularizer = regularization.TwoModelsMSE(model, args.reg_wd).reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  ep         lr    tr_loss     tr_acc     te_nll    te_loss     te_acc  ens_acc         time\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "   0   0.005000   0.010662  43.026000   0.008980   2.357947  39.270000              9.365962\n",
      "   1   0.005000   0.008132  42.954000   0.008250   2.357828  39.280000              9.153276\n",
      "   2   0.005000   0.007634  42.938000   0.007910   2.357758  39.270000              9.637742\n",
      "   3   0.005000   0.007399  42.956000   0.007742   2.357715  39.250000              9.375379\n",
      "   4   0.005000   0.007276  42.934000   0.007654   2.357688  39.220000             10.174862\n",
      "   5   0.005000   0.007214  42.926000   0.007605   2.357670  39.240000             10.967773\n",
      "   6   0.005000   0.007178  42.950000   0.007574   2.357675  39.240000             10.550008\n",
      "   7   0.005000   0.007156  42.930000   0.007555   2.357668  39.240000             10.254787\n",
      "   8   0.005000   0.007141  42.928000   0.007544   2.357662  39.250000              9.708352\n",
      "   9   0.005000   0.007131  42.942000   0.007533   2.357665  39.250000              9.206051\n",
      "  10   0.005000   0.007122  42.938000   0.007525   2.357668  39.230000              9.155132\n",
      "  11   0.005000   0.007117  42.912000   0.007520   2.357669  39.240000              8.671460\n",
      "  12   0.005000   0.007113  42.900000   0.007516   2.357669  39.240000              8.768783\n",
      "  13   0.005000   0.007109  42.918000   0.007513   2.357669  39.230000              8.331465\n",
      "  14   0.005000   0.007107  42.944000   0.007511   2.357669  39.240000              8.902208\n",
      "  15   0.005000   0.007105  42.914000   0.007507   2.357670  39.230000              9.453131\n",
      "  16   0.005000   0.007102  42.946000   0.007505   2.357671  39.230000              8.480820\n",
      "  17   0.005000   0.007101  42.934000   0.007502   2.357672  39.240000              9.012130\n",
      "  18   0.005000   0.007099  42.908000   0.007501   2.357670  39.230000              8.698655\n",
      "  19   0.005000   0.007097  42.922000   0.007499   2.357672  39.220000              8.444090\n",
      "  20   0.005000   0.007096  42.916000   0.007497   2.357673  39.220000              9.347487\n",
      "  21   0.005000   0.007095  42.914000   0.007496   2.357672  39.220000              8.935621\n",
      "  22   0.005000   0.007094  42.918000   0.007494   2.357673  39.220000              9.040092\n",
      "  23   0.005000   0.007093  42.910000   0.007493   2.357673  39.230000              9.536062\n",
      "  24   0.005000   0.007092  42.926000   0.007491   2.357674  39.230000              8.688583\n",
      "  25   0.005000   0.007091  42.908000   0.007490   2.357673  39.240000              8.716454\n",
      "  26   0.005000   0.007090  42.916000   0.007490   2.357673  39.230000              8.958163\n",
      "  27   0.005000   0.007090  42.914000   0.007487   2.357675  39.230000              8.509246\n",
      "  28   0.005000   0.007088  42.922000   0.007486   2.357674  39.240000              9.285461\n",
      "  29   0.005000   0.007087  42.922000   0.007485   2.357676  39.240000              8.424691\n",
      "  30   0.005000   0.007087  42.918000   0.007484   2.357676  39.240000              9.479650\n",
      "  31   0.005000   0.007086  42.928000   0.007482   2.357677  39.240000              9.093477\n",
      "  32   0.005000   0.007086  42.910000   0.007482   2.357677  39.240000              9.012725\n",
      "  33   0.005000   0.007085  42.912000   0.007481   2.357678  39.240000             10.571334\n",
      "  34   0.005000   0.007084  42.928000   0.007480   2.357679  39.250000              9.879874\n",
      "  35   0.005000   0.007084  42.918000   0.007479   2.357680  39.260000              9.869557\n",
      "  36   0.005000   0.007083  42.924000   0.007479   2.357681  39.250000              8.894821\n",
      "  37   0.005000   0.007083  42.924000   0.007478   2.357681  39.250000              9.236141\n",
      "  38   0.005000   0.007082  42.938000   0.007478   2.357682  39.250000              8.865141\n",
      "  39   0.005000   0.007082  42.922000   0.007477   2.357683  39.260000              9.229111\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  ep         lr    tr_loss     tr_acc     te_nll    te_loss     te_acc  ens_acc         time\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  40   0.005000   0.007081  42.928000   0.007476   2.357684  39.260000              9.946044\n",
      "  41   0.005000   0.007081  42.924000   0.007475   2.357685  39.260000              9.727309\n",
      "  42   0.005000   0.007081  42.924000   0.007475   2.357685  39.270000              9.613204\n",
      "  43   0.005000   0.007080  42.936000   0.007474   2.357686  39.270000             10.052852\n",
      "  44   0.005000   0.007080  42.924000   0.007473   2.357686  39.270000              9.658623\n",
      "  45   0.005000   0.007079  42.928000   0.007473   2.357686  39.270000              9.249202\n",
      "  46   0.005000   0.007079  42.938000   0.007473   2.357688  39.260000              9.193443\n",
      "  47   0.005000   0.007079  42.922000   0.007472   2.357689  39.270000              9.456435\n",
      "  48   0.005000   0.007078  42.928000   0.007471   2.357690  39.270000              8.767759\n",
      "  49   0.005000   0.007078  42.934000   0.007471   2.357690  39.270000              8.673190\n",
      "  50   0.005000   0.007078  42.908000   0.007470   2.357691  39.270000              8.690943\n",
      "  51   0.005000   0.007077  42.932000   0.007470   2.357692  39.270000              8.825999\n",
      "  52   0.005000   0.007077  42.934000   0.007469   2.357693  39.270000              8.838273\n",
      "  53   0.005000   0.007077  42.918000   0.007469   2.357693  39.270000              9.427033\n",
      "  54   0.005000   0.007077  42.942000   0.007468   2.357695  39.270000              8.655975\n",
      "  55   0.005000   0.007077  42.918000   0.007468   2.357695  39.270000              8.948465\n",
      "  56   0.005000   0.007076  42.920000   0.007468   2.357696  39.270000              9.581341\n",
      "  57   0.005000   0.007076  42.942000   0.007467   2.357697  39.270000              8.681400\n",
      "  58   0.005000   0.007076  42.922000   0.007467   2.357698  39.270000              8.496596\n",
      "  59   0.005000   0.007076  42.922000   0.007467   2.357697  39.270000              9.504468\n",
      "  60   0.005000   0.007076  42.922000   0.007466   2.357699  39.270000              9.300167\n",
      "  61   0.005000   0.007075  42.924000   0.007466   2.357700  39.270000              9.238989\n",
      "  62   0.005000   0.007075  42.926000   0.007466   2.357700  39.270000              9.686431\n",
      "  63   0.005000   0.007075  42.916000   0.007465   2.357700  39.270000             10.300891\n",
      "  64   0.005000   0.007074  42.932000   0.007465   2.357702  39.270000             10.087256\n",
      "  65   0.005000   0.007074  42.914000   0.007465   2.357702  39.270000              9.222710\n",
      "  66   0.005000   0.007074  42.924000   0.007465   2.357702  39.270000              9.157679\n",
      "  67   0.005000   0.007074  42.922000   0.007464   2.357703  39.270000              9.300340\n",
      "  68   0.005000   0.007074  42.924000   0.007464   2.357704  39.270000              8.363000\n",
      "  69   0.005000   0.007074  42.918000   0.007464   2.357703  39.270000              8.807475\n",
      "  70   0.005000   0.007074  42.912000   0.007463   2.357705  39.270000              8.936741\n",
      "  71   0.005000   0.007074  42.928000   0.007463   2.357704  39.270000              8.637831\n",
      "  72   0.005000   0.007073  42.924000   0.007463   2.357706  39.270000              9.238084\n",
      "  73   0.005000   0.007073  42.926000   0.007463   2.357706  39.270000              9.176435\n",
      "  74   0.005000   0.007073  42.922000   0.007463   2.357707  39.270000              8.749642\n",
      "  75   0.005000   0.007073  42.918000   0.007462   2.357708  39.270000              8.902502\n",
      "  76   0.005000   0.007073  42.922000   0.007462   2.357709  39.270000              9.044243\n",
      "  77   0.005000   0.007073  42.926000   0.007462   2.357708  39.270000              9.987418\n",
      "  78   0.005000   0.007073  42.910000   0.007462   2.357709  39.260000              9.801963\n",
      "  79   0.005000   0.007073  42.922000   0.007462   2.357710  39.270000              9.761109\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  ep         lr    tr_loss     tr_acc     te_nll    te_loss     te_acc  ens_acc         time\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  80   0.005000   0.007073  42.930000   0.007461   2.357710  39.270000              9.793270\n",
      "  81   0.005000   0.007072  42.926000   0.007461   2.357711  39.270000              9.359871\n",
      "  82   0.005000   0.007072  42.924000   0.007461   2.357712  39.270000              9.178334\n",
      "  83   0.005000   0.007072  42.924000   0.007461   2.357712  39.270000              9.361224\n",
      "  84   0.005000   0.007072  42.922000   0.007461   2.357712  39.260000              8.292424\n",
      "  85   0.005000   0.007072  42.924000   0.007461   2.357712  39.260000              8.994435\n",
      "  86   0.005000   0.007072  42.934000   0.007460   2.357713  39.260000              8.807385\n",
      "  87   0.005000   0.007072  42.920000   0.007460   2.357714  39.270000              8.840617\n",
      "  88   0.005000   0.007072  42.922000   0.007460   2.357714  39.260000              9.537183\n",
      "  89   0.005000   0.007072  42.914000   0.007460   2.357715  39.260000              9.340097\n",
      "  90   0.005000   0.007071  42.926000   0.007460   2.357716  39.260000              9.262715\n",
      "  91   0.005000   0.007072  42.920000   0.007460   2.357716  39.260000              9.456535\n",
      "  92   0.005000   0.007071  42.922000   0.007460   2.357716  39.260000             10.579561\n",
      "  93   0.005000   0.007071  42.924000   0.007460   2.357716  39.260000             10.076735\n",
      "  94   0.005000   0.007071  42.922000   0.007460   2.357717  39.260000             10.091599\n",
      "  95   0.005000   0.007071  42.920000   0.007459   2.357717  39.260000              9.761257\n",
      "  96   0.005000   0.007071  42.922000   0.007459   2.357718  39.260000              8.868921\n",
      "  97   0.005000   0.007071  42.928000   0.007459   2.357718  39.260000              8.796296\n",
      "  98   0.005000   0.007071  42.916000   0.007459   2.357718  39.260000              9.495696\n",
      "  99   0.005000   0.007071  42.924000   0.007459   2.357719  39.260000              9.273148\n",
      " 100   0.004939   0.007071  42.924000   0.007459   2.357719  39.260000              8.463763\n",
      " 101   0.004877   0.007071  42.920000   0.007459   2.357720  39.260000              9.309618\n",
      " 102   0.004816   0.007071  42.918000   0.007459   2.357720  39.260000              8.727800\n",
      " 103   0.004755   0.007071  42.928000   0.007458   2.357721  39.260000              8.837532\n",
      " 104   0.004694   0.007071  42.916000   0.007458   2.357721  39.260000              9.256298\n",
      " 105   0.004632   0.007071  42.924000   0.007458   2.357721  39.260000              8.936223\n",
      " 106   0.004571   0.007070  42.922000   0.007458   2.357722  39.260000              8.534516\n",
      " 107   0.004510   0.007070  42.922000   0.007458   2.357722  39.260000              8.935841\n",
      " 108   0.004449   0.007071  42.930000   0.007458   2.357722  39.250000              8.767096\n",
      " 109   0.004387   0.007070  42.918000   0.007458   2.357723  39.250000              9.819177\n",
      " 110   0.004326   0.007070  42.932000   0.007458   2.357723  39.260000              9.372984\n",
      " 111   0.004265   0.007070  42.904000   0.007458   2.357723  39.250000              8.664605\n",
      " 112   0.004204   0.007070  42.928000   0.007458   2.357724  39.250000              9.790965\n",
      " 113   0.004143   0.007070  42.924000   0.007458   2.357724  39.250000              9.842847\n",
      " 114   0.004081   0.007070  42.922000   0.007458   2.357724  39.250000             10.127619\n",
      " 115   0.004020   0.007070  42.924000   0.007458   2.357724  39.250000              9.913335\n",
      " 116   0.003959   0.007070  42.916000   0.007458   2.357725  39.250000              9.595042\n",
      " 117   0.003898   0.007070  42.924000   0.007458   2.357725  39.250000              8.744449\n",
      " 118   0.003836   0.007070  42.930000   0.007458   2.357725  39.250000              8.980472\n",
      " 119   0.003775   0.007070  42.930000   0.007457   2.357725  39.250000              9.058569\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  ep         lr    tr_loss     tr_acc     te_nll    te_loss     te_acc  ens_acc         time\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      " 120   0.003714   0.007070  42.938000   0.007457   2.357726  39.240000              8.962129\n",
      " 121   0.003653   0.007070  42.930000   0.007457   2.357726  39.240000             10.091710\n",
      " 122   0.003591   0.007070  42.924000   0.007457   2.357726  39.250000              9.991427\n",
      " 123   0.003530   0.007070  42.926000   0.007457   2.357726  39.240000              8.611035\n",
      " 124   0.003469   0.007070  42.928000   0.007457   2.357727  39.240000              9.310820\n",
      " 125   0.003407   0.007070  42.928000   0.007457   2.357727  39.240000              9.465820\n",
      " 126   0.003346   0.007070  42.928000   0.007457   2.357727  39.240000              8.905856\n",
      " 127   0.003285   0.007070  42.928000   0.007457   2.357727  39.240000              8.827874\n",
      " 128   0.003224   0.007070  42.926000   0.007457   2.357728  39.240000              9.217722\n",
      " 129   0.003162   0.007070  42.930000   0.007457   2.357727  39.240000              8.950973\n",
      " 130   0.003101   0.007069  42.922000   0.007457   2.357728  39.240000              8.576543\n",
      " 131   0.003040   0.007070  42.920000   0.007457   2.357728  39.240000              8.876954\n",
      " 132   0.002979   0.007070  42.920000   0.007457   2.357728  39.240000              9.123557\n",
      " 133   0.002917   0.007070  42.926000   0.007457   2.357728  39.240000              8.824430\n",
      " 134   0.002856   0.007070  42.930000   0.007457   2.357728  39.240000              8.567564\n",
      " 135   0.002795   0.007070  42.934000   0.007457   2.357729  39.240000              9.662192\n",
      " 136   0.002734   0.007070  42.924000   0.007457   2.357729  39.240000              9.465806\n",
      " 137   0.002673   0.007069  42.926000   0.007457   2.357729  39.240000              9.434904\n",
      " 138   0.002611   0.007070  42.916000   0.007457   2.357729  39.240000              8.727498\n",
      " 139   0.002550   0.007069  42.920000   0.007457   2.357729  39.240000              8.942531\n",
      " 140   0.002489   0.007069  42.928000   0.007457   2.357729  39.240000              9.443343\n",
      " 141   0.002428   0.007070  42.920000   0.007457   2.357729  39.240000              8.722267\n",
      " 142   0.002366   0.007069  42.914000   0.007457   2.357729  39.240000              8.469652\n",
      " 143   0.002305   0.007070  42.914000   0.007457   2.357730  39.240000              9.413041\n",
      " 144   0.002244   0.007069  42.920000   0.007457   2.357730  39.240000              9.184189\n",
      " 145   0.002183   0.007069  42.930000   0.007457   2.357730  39.240000              9.386002\n",
      " 146   0.002121   0.007069  42.926000   0.007457   2.357730  39.240000              9.787360\n",
      " 147   0.002060   0.007069  42.926000   0.007457   2.357730  39.240000              9.740711\n",
      " 148   0.001999   0.007069  42.924000   0.007457   2.357730  39.240000              9.581691\n",
      " 149   0.001937   0.007070  42.928000   0.007456   2.357730  39.240000             10.716339\n",
      " 150   0.001876   0.007069  42.934000   0.007456   2.357730  39.240000             10.667192\n",
      " 151   0.001815   0.007069  42.924000   0.007456   2.357730  39.240000             10.417881\n",
      " 152   0.001754   0.007069  42.916000   0.007456   2.357730  39.240000             10.019718\n",
      " 153   0.001692   0.007069  42.936000   0.007456   2.357731  39.240000              9.785182\n",
      " 154   0.001631   0.007069  42.930000   0.007456   2.357731  39.240000              8.988074\n",
      " 155   0.001570   0.007069  42.928000   0.007456   2.357731  39.240000              8.639937\n",
      " 156   0.001509   0.007069  42.922000   0.007456   2.357731  39.240000              8.845346\n",
      " 157   0.001447   0.007069  42.918000   0.007456   2.357731  39.240000              9.493770\n",
      " 158   0.001386   0.007069  42.924000   0.007456   2.357731  39.240000              9.160400\n",
      " 159   0.001325   0.007069  42.924000   0.007456   2.357731  39.240000              8.669853\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  ep         lr    tr_loss     tr_acc     te_nll    te_loss     te_acc  ens_acc         time\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      " 160   0.001264   0.007069  42.932000   0.007456   2.357731  39.240000              9.415689\n",
      " 161   0.001202   0.007069  42.924000   0.007456   2.357731  39.240000              9.427669\n",
      " 162   0.001141   0.007069  42.930000   0.007456   2.357731  39.240000              9.162245\n",
      " 163   0.001080   0.007069  42.930000   0.007456   2.357731  39.240000              8.779219\n",
      " 164   0.001019   0.007069  42.930000   0.007456   2.357731  39.240000              8.907971\n",
      " 165   0.000958   0.007069  42.938000   0.007456   2.357731  39.240000              9.086515\n",
      " 166   0.000896   0.007069  42.920000   0.007456   2.357731  39.240000              8.423175\n",
      " 167   0.000835   0.007069  42.916000   0.007456   2.357731  39.240000              8.597757\n",
      " 168   0.000774   0.007069  42.920000   0.007456   2.357731  39.240000              8.801336\n",
      " 169   0.000713   0.007069  42.918000   0.007456   2.357731  39.240000              9.328641\n",
      " 170   0.000651   0.007069  42.924000   0.007456   2.357732  39.240000              9.116660\n",
      " 171   0.000590   0.007069  42.926000   0.007456   2.357732  39.240000              8.449219\n",
      " 172   0.000529   0.007069  42.924000   0.007456   2.357732  39.240000              9.645589\n",
      " 173   0.000468   0.007069  42.920000   0.007456   2.357732  39.240000              8.962432\n",
      " 174   0.000406   0.007069  42.924000   0.007456   2.357732  39.240000              8.497900\n",
      " 175   0.000345   0.007069  42.914000   0.007456   2.357732  39.240000              9.103994\n",
      " 176   0.000284   0.007069  42.920000   0.007456   2.357732  39.240000              9.718512\n",
      " 177   0.000223   0.007069  42.926000   0.007456   2.357732  39.240000              8.720228\n",
      " 178   0.000161   0.007069  42.924000   0.007456   2.357732  39.240000              9.414799\n",
      " 179   0.000100   0.007069  42.916000   0.007456   2.357732  39.240000              9.271836\n",
      " 180   0.000100   0.007069  42.920000   0.007456   2.357732  39.240000              9.984091\n",
      " 181   0.000100   0.007069  42.922000   0.007456   2.357732  39.240000              8.877866\n",
      " 182   0.000100   0.007069  42.922000   0.007456   2.357732  39.240000             10.550508\n",
      " 183   0.000100   0.007069  42.918000   0.007456   2.357732  39.240000             10.642764\n",
      " 184   0.000100   0.007069  42.932000   0.007456   2.357732  39.240000              9.801661\n",
      " 185   0.000100   0.007069  42.928000   0.007456   2.357732  39.240000              9.430641\n",
      " 186   0.000100   0.007069  42.926000   0.007456   2.357732  39.240000              9.896983\n",
      " 187   0.000100   0.007069  42.924000   0.007456   2.357732  39.240000              8.926870\n",
      " 188   0.000100   0.007069  42.914000   0.007456   2.357732  39.240000              9.659736\n",
      " 189   0.000100   0.007069  42.928000   0.007456   2.357732  39.240000              8.927140\n",
      " 190   0.000100   0.007069  42.928000   0.007456   2.357732  39.240000              8.756105\n",
      " 191   0.000100   0.007069  42.936000   0.007456   2.357732  39.240000              8.962424\n",
      " 192   0.000100   0.007069  42.918000   0.007456   2.357732  39.240000              8.455528\n",
      " 193   0.000100   0.007069  42.930000   0.007456   2.357732  39.240000              9.324710\n",
      " 194   0.000100   0.007069  42.930000   0.007456   2.357732  39.240000              8.822674\n",
      " 195   0.000100   0.007069  42.932000   0.007456   2.357732  39.240000              8.701326\n",
      " 196   0.000100   0.007069  42.922000   0.007456   2.357732  39.240000              8.690180\n",
      " 197   0.000100   0.007069  42.918000   0.007456   2.357732  39.240000              8.646629\n",
      " 198   0.000100   0.007069  42.916000   0.007456   2.357732  39.240000              9.609688\n",
      "[ 99 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 199 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 299 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 399 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 499 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 599 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 699 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 799 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 899 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "[ 999 ] lr : tensor([17.6585], requires_grad=True) grad : tensor([0.])\n",
      "I am saving to ../Checkpoints/FGE/CIFAR100/ConvFCSimpleTanh/ind_gb/classic/0/boost_lr/199.pt\n",
      "Boost_lr :  17.658540725708008\n",
      "+---------------\n",
      "|Mean : 0.00010792190005304292 Std 0.0011742201168090105\n",
      "|Max : 0.015114428475499153 Min : -0.014795435592532158\n",
      "|Sum Mean : 0.03153901547193527 Sum Std 3.4236278533935547\n",
      "|Sum Max : 27.49024200439453 |Sum Min : -29.317237854003906\n",
      "+--------------\n",
      "I am making a new model\n",
      " 199   0.000100   0.007069  42.930000   0.007456   2.357732  39.240000  42.449999   8.463223\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  ep         lr    tr_loss     tr_acc     te_nll    te_loss     te_acc  ens_acc         time\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      " 200   0.005000   0.011469  35.994000   0.009050   2.534689  35.790000             10.717728\n",
      " 201   0.005000   0.008203  39.814000   0.008223   2.445860  37.530000              9.361614\n",
      " 202   0.005000   0.007644  41.296000   0.007872   2.406997  38.460000              9.273318\n",
      " 203   0.005000   0.007398  41.928000   0.007711   2.389049  38.520000              8.625921\n",
      " 204   0.005000   0.007277  42.380000   0.007629   2.379419  38.740000              9.210665\n",
      " 205   0.005000   0.007214  42.416000   0.007585   2.373917  39.000000              9.876528\n",
      " 206   0.005000   0.007179  42.646000   0.007560   2.370794  38.920000              8.235731\n",
      " 207   0.005000   0.007157  42.746000   0.007543   2.368694  39.050000              9.819880\n",
      " 208   0.005000   0.007142  42.722000   0.007531   2.367210  39.120000             10.417150\n",
      " 209   0.005000   0.007134  42.774000   0.007523   2.366093  39.110000             10.089743\n",
      " 210   0.005000   0.007125  42.766000   0.007516   2.365266  39.090000             10.030353\n",
      " 211   0.005000   0.007118  42.870000   0.007511   2.364573  39.120000              9.977452\n",
      " 212   0.005000   0.007114  42.854000   0.007506   2.363958  39.160000              9.460644\n",
      " 213   0.005000   0.007110  42.866000   0.007502   2.363493  39.130000              9.423012\n",
      " 214   0.005000   0.007107  42.768000   0.007500   2.363147  39.130000              9.058280\n",
      " 215   0.005000   0.007104  42.796000   0.007497   2.362818  39.170000              9.486653\n",
      " 216   0.005000   0.007101  42.886000   0.007494   2.362466  39.150000              8.591671\n",
      " 217   0.005000   0.007099  42.934000   0.007493   2.362277  39.170000              9.179946\n",
      " 218   0.005000   0.007097  42.870000   0.007490   2.361956  39.170000              9.744057\n",
      " 219   0.005000   0.007096  42.866000   0.007489   2.361805  39.170000              9.312032\n",
      " 220   0.005000   0.007095  42.862000   0.007487   2.361611  39.140000              8.902061\n",
      " 221   0.005000   0.007093  42.834000   0.007486   2.361415  39.170000              9.246913\n",
      " 222   0.005000   0.007092  42.954000   0.007484   2.361276  39.200000              9.407073\n",
      " 223   0.005000   0.007090  42.872000   0.007483   2.361068  39.200000              8.895982\n",
      " 224   0.005000   0.007090  42.910000   0.007482   2.360913  39.220000              8.985341\n",
      " 225   0.005000   0.007088  42.886000   0.007480   2.360759  39.220000              9.130184\n",
      " 226   0.005000   0.007088  42.914000   0.007479   2.360625  39.210000              9.852398\n",
      " 227   0.005000   0.007087  42.886000   0.007478   2.360463  39.190000             10.583666\n",
      " 228   0.005000   0.007086  42.904000   0.007477   2.360311  39.230000             10.345917\n",
      " 229   0.005000   0.007085  42.824000   0.007476   2.360250  39.230000             10.493997\n",
      " 230   0.005000   0.007084  42.942000   0.007475   2.360085  39.230000              8.930391\n",
      " 231   0.005000   0.007083  42.852000   0.007474   2.359976  39.240000              9.362470\n",
      " 232   0.005000   0.007082  42.894000   0.007473   2.359873  39.240000              8.841010\n",
      " 233   0.005000   0.007082  42.912000   0.007473   2.359759  39.280000              9.099614\n",
      " 234   0.005000   0.007081  42.930000   0.007472   2.359644  39.280000             10.198599\n",
      " 235   0.005000   0.007080  42.974000   0.007471   2.359556  39.250000              9.227230\n",
      " 236   0.005000   0.007079  42.908000   0.007470   2.359446  39.250000              9.259512\n",
      " 237   0.005000   0.007079  42.864000   0.007470   2.359375  39.260000              9.883669\n",
      " 238   0.005000   0.007079  42.906000   0.007469   2.359311  39.270000              8.954844\n",
      " 239   0.005000   0.007078  42.984000   0.007469   2.359255  39.270000              9.681601\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      "  ep         lr    tr_loss     tr_acc     te_nll    te_loss     te_acc  ens_acc         time\n",
      "----  ---------  ---------  ---------  ---------  ---------  ---------  ---------  ---------\n",
      " 240   0.005000   0.007078  42.900000   0.007468   2.359152  39.290000              9.058036\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    time_ep = time.time()\n",
    "    lr_schedule = scheduler(epoch, args.cycle, args.lr_1, args.lr_2)\n",
    "    \n",
    "    train_res = utils.train_gb(\n",
    "        loaders['train'],\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        lr_schedule=lr_schedule,\n",
    "        regularizer=regularizer,\n",
    "        gb_version=args.version,\n",
    "        boost_lr=boost_lr)\n",
    "    test_res = utils.test_gb(\n",
    "        loaders['test'],\n",
    "        model,\n",
    "        criterion,\n",
    "        boost_lr=boost_lr)\n",
    "    time_ep = time.time() - time_ep\n",
    "    ens_acc = None\n",
    "\n",
    "    if (epoch + 1) % args.cycle == 0:\n",
    "        if args.boost_lr == 'auto':\n",
    "            os.makedirs(args.dir + '/boost_lr', exist_ok=True)\n",
    "            boost_lr = regularization.adjust_boost_lr(\n",
    "                loaders['train'],\n",
    "                model,\n",
    "                save_info=args.dir + '/boost_lr/' + str(epoch) + '.pt')\n",
    "        print ('Boost_lr : ', boost_lr)\n",
    "        ensemble_size += 1\n",
    "        logits, targets = utils.logits(loaders['test'], model)\n",
    "        logits_sum += boost_lr * logits\n",
    "        ens_acc = 100.0 * torch.eq(logits_sum.argmax(dim=1), targets).float().mean().item()\n",
    "        \n",
    "        regularization.logits_info(logits, logits_sum=logits_sum)\n",
    "        \n",
    "        utils.save_checkpoint(\n",
    "            args.dir,\n",
    "            start_epoch + epoch,\n",
    "            name='fge',\n",
    "            model_state=model.state_dict(),\n",
    "            optimizer_state=optimizer.state_dict(),\n",
    "            boost_weight=boost_lr\n",
    "        )\n",
    "\n",
    "#     if args.regularizer is not None and (epoch + 1) % (args.cycle) == 0:\n",
    "#         regularizer = regularization.TwoModelsMSE(model, args.reg_wd).reg\n",
    "#     if args.regularizer is not None and (epoch + 1) % (args.cycle // 2) == args.cycle // 2:\n",
    "#         regularizer = None\n",
    "\n",
    "#     if args.weighted_samples is not None and (epoch + 1) % args.cycle == 0:\n",
    "#     if (epoch + 1) % args.cycle == 0:\n",
    "        loaders['train'].dataset.update_logits(\n",
    "            boost_lr,\n",
    "            logits_generator=regularization.dataset_logits_generator(\n",
    "                model,\n",
    "                transform=getattr(getattr(\n",
    "                        data.Transforms,\n",
    "                        args.dataset),\n",
    "                    args.transform).train,\n",
    "                batch_size = args.batch_size))\n",
    "        loaders['test'].dataset.update_logits(\n",
    "            boost_lr,\n",
    "            logits_generator=regularization.dataset_logits_generator(\n",
    "                model,\n",
    "                transform=getattr(getattr(\n",
    "                        data.Transforms,\n",
    "                        args.dataset),\n",
    "                    args.transform).test,\n",
    "                batch_size = args.batch_size))\n",
    "        \n",
    "        if args.independent:\n",
    "            print (\"I am making a new model\")\n",
    "            model = architecture.base(num_classes=num_classes, **architecture.kwargs)\n",
    "            model.cuda()\n",
    "            optimizer = torch.optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=args.lr_1,\n",
    "                momentum=args.momentum,\n",
    "                weight_decay=args.wd\n",
    "            )\n",
    "        \n",
    "    values = [epoch, lr_schedule(1.0), train_res['loss'], train_res['accuracy'], test_res['nll'], test_res['loss'], test_res['accuracy'], ens_acc, time_ep]\n",
    "    table = tabulate.tabulate([values], columns, tablefmt='simple', floatfmt='9.6f')\n",
    "    if epoch % 40 == 0:\n",
    "        table = table.split('\\n')\n",
    "        table = '\\n'.join([table[1]] + table)\n",
    "    else:\n",
    "        table = table.split('\\n')[2]\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
